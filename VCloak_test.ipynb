{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fd9cd57-edaa-4e0e-a3ec-28ade2af287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "from speechbrain.inference.speaker import EncoderClassifier\n",
    "\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "\n",
    "reference, rate = torchaudio.load('./Datasets/audio_to_anon/LibriSpeech/dev-clean/1462/170138/1462-170138-0001.flac')\n",
    "reference_emb = classifier.encode_batch(reference)\n",
    "\n",
    "speaker, _ = torchaudio.load('./Datasets/audio_to_anon/LibriSpeech/dev-clean/2086/149214/2086-149214-0000.flac')\n",
    "speaker_emb = classifier.encode_batch(speaker)\n",
    "\n",
    "anonim, _ = torchaudio.load('./converted_sound_Waveunet_test/*.flac/1462/170138/1462-170138-0000.flac')\n",
    "anonim_emb = classifier.encode_batch(anonim)\n",
    "\n",
    "room, _ = torchaudio.load('./add_room.wav')\n",
    "room_emb = classifier.encode_batch(room)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce143e23-fcfb-4ce4-969d-5e526190507f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0579]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(reference_emb, speaker_emb, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b075e3a3-5daa-4f5c-80a4-1680e1935d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2726]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(reference_emb, anonim_emb, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea93a258-6254-410d-b2f5-577b25c4b2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2385]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(reference_emb, room_emb, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12411ef-5668-414a-9ce9-6f6fc44d8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchaudio.save('processed_audio.wav', anonim, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b0d3b-778d-48bb-9463-296fcbbfd849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8624243f-8145-44f0-9375-ad939764dab6",
   "metadata": {},
   "source": [
    "# Verification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17830d73-bac9-4f5e-ae04-8f34afbf7ade",
   "metadata": {},
   "source": [
    "## ECAPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8cb2282-7c71-4213-92b6-8ec3abb22989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from speechbrain.inference.speaker import EncoderClassifier\n",
    "import re\n",
    "import glob\n",
    "from tqdm.auto import tqdm \n",
    "from IPython.display import clear_output\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2acbd024-2382-4936-87b6-5c839bd28b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_id(file_path: str, pattern: str = None) -> str:\n",
    "    pattern = pattern or r'\\/(\\d{2,4})\\/'\n",
    "    return re.search(pattern, file_path).group(1)\n",
    "\n",
    "def parse_ids(file_paths: list[str]) -> list[str]:\n",
    "    return list(set(find_id(file_path) for file_path in file_paths))\n",
    "\n",
    "def find_all_speaker_audio(folder: str, speaker_id: str) -> list[str]:\n",
    "    return glob.glob(folder + speaker_id + '/*/*.flac')\n",
    "\n",
    "def load_audios(speaker_files: list[str]) -> list[torch.Tensor]:\n",
    "    audios = []\n",
    "    \n",
    "    for file in speaker_files:\n",
    "        audio, rate = torchaudio.load(file)\n",
    "        assert rate == 16000\n",
    "        audios.append(audio.to(device))\n",
    "    \n",
    "    return audios\n",
    "\n",
    "def check_verification_model(model: torch.nn.Module, speaker_id: str, folder: str, num_samples: int = 10, threshhold: float = 0.25):\n",
    "    speaker_files = find_all_speaker_audio(folder, speaker_id)\n",
    "    \n",
    "    speaker_files_subset = list(np.random.choice(speaker_files, num_samples, replace=False))\n",
    "    referense_file = np.random.choice(find_all_speaker_audio(original_folder, speaker_id))\n",
    "    \n",
    "    audios = load_audios(speaker_files)\n",
    "    reference_audio, _ = torchaudio.load(referense_file)\n",
    "    \n",
    "    reference_embedding = model.encode_batch(reference_audio)\n",
    "    \n",
    "    speaker_embeddings = []\n",
    "    for audio in audios:\n",
    "        speaker_embeddings.append(model.encode_batch(audio))\n",
    "    \n",
    "    scores = []\n",
    "    for speaker_embedding in speaker_embeddings:\n",
    "        scores.append(F.cosine_similarity(speaker_embedding, reference_embedding, dim=2).cpu().item())\n",
    "        \n",
    "    scores = np.array(scores)\n",
    "    accuracy = (scores > threshhold).sum() / len(scores)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3794925-34c7-4808-bd05-7e176cf5e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audios: 2703,\n",
      "Number of speakers: 40\n"
     ]
    }
   ],
   "source": [
    "converted_files = glob.glob('./converted_sound_Waveunet_test/*.flac/*/*/*.flac')\n",
    "ids = parse_ids(converted_files)\n",
    "\n",
    "print(f'Number of audios: {len(converted_files)},\\nNumber of speakers: {len(ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873dbe3b-b276-42b5-a3bc-4f4b59fb8044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "converted_folder = './converted_sound_Waveunet_test/*.flac/'\n",
    "original_folder = './Datasets/audio_to_anon/LibriSpeech/dev-clean/'\n",
    "rir_folder = './processed/'\n",
    "\n",
    "num_samples = 10\n",
    "device = 'cuda:0'\n",
    "\n",
    "scores_by_ids = dict.fromkeys(ids)\n",
    "scores_by_ids_anonim = dict.fromkeys(ids)\n",
    "scores_by_ids_rir = dict.fromkeys(ids)\n",
    "\n",
    "model = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", run_opts={\"device\": device})\n",
    "\n",
    "for i, speaker_id in enumerate(ids):\n",
    "    # my tqdm\n",
    "    clear_output(wait=False)\n",
    "    print(i)\n",
    "    \n",
    "    scores_by_ids[speaker_id] = check_verification_model(model, speaker_id, original_folder, threshhold=0.3, num_samples=20)\n",
    "    scores_by_ids_anonim[speaker_id] = check_verification_model(model, speaker_id, converted_folder, threshhold=0.3, num_samples=20)\n",
    "    scores_by_ids_rir[speaker_id] = check_verification_model(model, speaker_id, rir_folder, threshhold=0.3, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ed5bf10-78d6-491a-9814-89a87b335c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy: 0.99\n",
      "Anonimized accuracy: 0.12\n",
      "RIR accuracy: 0.05\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial accuracy: {np.mean(list(scores_by_ids.values())):.2f}')\n",
    "print(f'Anonimized accuracy: {np.mean(list(scores_by_ids_anonim.values())):.2f}')\n",
    "print(f'RIR accuracy: {np.mean(list(scores_by_ids_rir.values())):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aacda8-4978-4adb-be67-acb82c36fed4",
   "metadata": {},
   "source": [
    "## Pyannonte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd10e021-a2bc-4420-972d-768ab59d81ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/pyannote/models--pyannote--embedding/snapshots/4db4899737a38b2d618bbd74350915aa10293cb2/pytorch_model.bin`\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.2.7 to v2.2.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/pyannote/models--pyannote--embedding/snapshots/4db4899737a38b2d618bbd74350915aa10293cb2/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.2.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.3.0+cu118. Bad things might happen unless you revert torch to 1.x.\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.2.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.8.1+cu102, yours is 2.3.0+cu118. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Model\n",
    "model = Model.from_pretrained(\"pyannote/embedding\", \n",
    "                              use_auth_token=\"your_hugginface_token\")\n",
    "\n",
    "from pyannote.audio import Inference\n",
    "inference = Inference(model, window=\"whole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c633ca73-f2aa-4414-8dda-5cc2ca2b1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_verification_model(model: torch.nn.Module, speaker_id: str, folder: str, num_samples: int = 10, threshhold: float = 0.25):\n",
    "    with torch.no_grad():\n",
    "        speaker_files = find_all_speaker_audio(folder, speaker_id)\n",
    "\n",
    "        speaker_files_subset = list(np.random.choice(speaker_files, num_samples, replace=False))\n",
    "        referense_file = np.random.choice(find_all_speaker_audio(original_folder, speaker_id))\n",
    "\n",
    "        audios = load_audios(speaker_files)\n",
    "        reference_audio, _ = torchaudio.load(referense_file)\n",
    "\n",
    "        reference_embedding = model(reference_audio.to(device))\n",
    "\n",
    "        speaker_embeddings = []\n",
    "        for audio in audios:\n",
    "            speaker_embeddings.append(model(audio))\n",
    "\n",
    "        scores = []\n",
    "        for speaker_embedding in speaker_embeddings:\n",
    "            scores.append(F.cosine_similarity(speaker_embedding, reference_embedding, dim=1).cpu().item())\n",
    "\n",
    "        scores = np.array(scores)\n",
    "        accuracy = (scores > threshhold).sum() / len(scores)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9dd465d4-bbc3-4179-91bb-be3113456317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "converted_folder = './converted_sound_Waveunet_test/*.flac/'\n",
    "original_folder = './Datasets/audio_to_anon/LibriSpeech/dev-clean/'\n",
    "rir_folder = './processed/'\n",
    "\n",
    "num_samples = 10\n",
    "device = 'cuda:0'\n",
    "\n",
    "scores_by_ids = dict.fromkeys(ids)\n",
    "scores_by_ids_anonim = dict.fromkeys(ids)\n",
    "scores_by_ids_rir = dict.fromkeys(ids)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for i, speaker_id in enumerate(ids):\n",
    "    # my tqdm\n",
    "    clear_output(wait=False)\n",
    "    print(i)\n",
    "    \n",
    "    scores_by_ids[speaker_id] = check_verification_model(model, speaker_id, original_folder, threshhold=0.3, num_samples=20)\n",
    "    scores_by_ids_anonim[speaker_id] = check_verification_model(model, speaker_id, converted_folder, threshhold=0.3, num_samples=20)\n",
    "    scores_by_ids_rir[speaker_id] = check_verification_model(model, speaker_id, rir_folder, threshhold=0.3, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e41090d3-d032-4c44-9804-6e525ec11a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy: 0.97\n",
      "Anonimized accuracy: 0.07\n",
      "RIR accuracy: 0.02\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial accuracy: {np.mean(list(scores_by_ids.values())):.2f}')\n",
    "print(f'Anonimized accuracy: {np.mean(list(scores_by_ids_anonim.values())):.2f}')\n",
    "print(f'RIR accuracy: {np.mean(list(scores_by_ids_rir.values())):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df8f0d-272a-483c-9f0b-f17cae8abcb0",
   "metadata": {},
   "source": [
    "# Wespeaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb3243f-6706-42a0-8cb5-feb0ef5bc522",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:unexpected tensor: projection.weight\n"
     ]
    }
   ],
   "source": [
    "import wespeaker\n",
    "\n",
    "model = wespeaker.load_model('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deca25b6-1012-4836-845b-f2214bfd71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wespeaker_inference(model: torch.nn.Module, sample: torch.Tensor) -> torch.Tensor:\n",
    "    with torch.no_grad():\n",
    "        sample = model.compute_fbank(sample, sample_rate=model.resample_rate, cmn=True)\n",
    "        sample = sample.unsqueeze(0)\n",
    "        sample = sample.to(model.device)\n",
    "        outputs = model.model(sample)\n",
    "        outputs = outputs[-1] if isinstance(outputs, tuple) else outputs\n",
    "        embed = outputs[0]\n",
    "    return embed\n",
    "    \n",
    "\n",
    "def check_verification_model(model: torch.nn.Module, speaker_id: str, folder: str, num_samples: int = 10, threshhold: float = 0.25):\n",
    "    speaker_files = find_all_speaker_audio(folder, speaker_id)\n",
    "    \n",
    "    speaker_files_subset = list(np.random.choice(speaker_files, num_samples, replace=False))\n",
    "    referense_file = np.random.choice(find_all_speaker_audio(original_folder, speaker_id))\n",
    "    \n",
    "    audios = load_audios(speaker_files)\n",
    "    reference_audio, _ = torchaudio.load(referense_file)\n",
    "    \n",
    "    reference_embedding = wespeaker_inference(model, reference_audio)\n",
    "    \n",
    "    speaker_embeddings = []\n",
    "    for audio in audios:\n",
    "        speaker_embeddings.append(wespeaker_inference(model, audio))\n",
    "    \n",
    "    scores = []\n",
    "    for speaker_embedding in speaker_embeddings:\n",
    "        scores.append(F.cosine_similarity(speaker_embedding.unsqueeze(0), reference_embedding.unsqueeze(0), dim=1).cpu().item())\n",
    "        \n",
    "    scores = np.array(scores)\n",
    "    accuracy = (scores > threshhold).sum() / len(scores)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e802ad9-c8ce-4e4b-9e2e-68f06f6a4141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "converted_folder = './converted_sound_Waveunet_test/*.flac/'\n",
    "original_folder = './Datasets/audio_to_anon/LibriSpeech/dev-clean/'\n",
    "rir_folder = './processed/'\n",
    "\n",
    "scores_by_ids = dict.fromkeys(ids)\n",
    "scores_by_ids_anonim = dict.fromkeys(ids)\n",
    "scores_by_ids_rir = dict.fromkeys(ids)\n",
    "\n",
    "for i, speaker_id in enumerate(ids):\n",
    "    # my tqdm\n",
    "    clear_output(wait=False)\n",
    "    print(i)\n",
    "    \n",
    "    scores_by_ids[speaker_id] = check_verification_model(model, speaker_id, original_folder, threshhold=0.3, num_samples=20)\n",
    "    scores_by_ids_anonim[speaker_id] = check_verification_model(model, speaker_id, converted_folder, threshhold=0.3, num_samples=20)\n",
    "    scores_by_ids_rir[speaker_id] = check_verification_model(model, speaker_id, rir_folder, threshhold=0.3, num_samples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c5d2e5e-3a6a-4c9d-a86b-1dedd23216d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy: 0.99\n",
      "Anonimized accuracy: 0.44\n",
      "RIR accuracy: 0.26\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial accuracy: {np.mean(list(scores_by_ids.values())):.2f}')\n",
    "print(f'Anonimized accuracy: {np.mean(list(scores_by_ids_anonim.values())):.2f}')\n",
    "print(f'RIR accuracy: {np.mean(list(scores_by_ids_rir.values())):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f338d25-9ca4-4bb0-8f17-e1a5e846a869",
   "metadata": {},
   "source": [
    "# WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "152c9114-ac35-4293-bb0e-274af85eb6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8053464c-a0ef-42b3-87b5-a66bfc308310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/vcloak/lib/python3.9/site-packages/transformers/generation/utils.py:1298: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mister quilter is the apostle of the middle classes and we are glad to welcome his gospel']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
    "import torchaudio\n",
    "\n",
    "model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "audio, _ = torchaudio.load('./Datasets/audio_to_anon/LibriSpeech/dev-clean/1272/128104/1272-128104-0000.flac')\n",
    "inputs = processor(audio.squeeze(), sampling_rate=16_000, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(inputs[\"input_features\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device))\n",
    "\n",
    "transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "592579dd-17ef-48a2-9a8e-a19681a51e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from evaluate import load\n",
    "wer = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35874b96-3970-47f7-a76d-630ba7e412e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(path: str) -> pd.DataFrame:\n",
    "    texts = pd.read_csv(path, header=None)\n",
    "    texts.columns = ['text']\n",
    "    texts[['code', 'transcription']] = texts.text.str.split(' ', n=1, expand=True)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def transcribe_audio(path_to_audio: str) -> str:\n",
    "    audio, _ = torchaudio.load(path_to_audio)\n",
    "    inputs = processor(audio.squeeze(), sampling_rate=16_000, return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(inputs[\"input_features\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device))\n",
    "\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "211fb766-22ae-4132-ada6-04db1609619c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/vcloak/lib/python3.9/site-packages/transformers/generation/utils.py:1298: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WER: 0.03800558941382623\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "first_folders = glob.glob('./Datasets/audio_to_anon/LibriSpeech/dev-clean/*')\n",
    "\n",
    "wer_score = 0\n",
    "num_files = 0\n",
    "\n",
    "for i, first_folder in enumerate(first_folders):\n",
    "    clear_output(wait=False)\n",
    "    print(i)\n",
    "    \n",
    "    second_folders = glob.glob(first_folder + '/*')\n",
    "\n",
    "    for second_folder in second_folders:\n",
    "    \n",
    "        third_folders = glob.glob(second_folder + '/*.flac')\n",
    "        text_file = glob.glob(second_folder + '/*.txt')[0]\n",
    "        \n",
    "        texts_df = load_txt(text_file)\n",
    "        \n",
    "        for audio_path in third_folders:\n",
    "            prediction = transcribe_audio(audio_path)[0]\n",
    "            \n",
    "            code = re.search(r'/([^/]+)\\.flac$', audio_path).group(1)\n",
    "            reference = texts_df.loc[texts_df.code == code].transcription.tolist()[0]\n",
    "            \n",
    "            # print(f'Reference: {reference.lower()}')\n",
    "            # print(f'Prediction: {prediction[0].lower()}')\n",
    "            \n",
    "            wer_score += wer(reference.lower(), prediction.lower())\n",
    "            num_files += 1\n",
    "\n",
    "average_wer = wer_score / num_files\n",
    "print(f\"Average WER: {average_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7f148a4-70e0-40cd-9239-72685ea7c668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/vcloak/lib/python3.9/site-packages/transformers/generation/utils.py:1298: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WER: 0.14456822142189207\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "first_folders = glob.glob('./converted_sound_Waveunet_test/*.flac/*')\n",
    "\n",
    "wer_score = 0\n",
    "num_files = 0\n",
    "\n",
    "for i, first_folder in enumerate(first_folders):\n",
    "    clear_output(wait=False)\n",
    "    print(i)\n",
    "    \n",
    "    second_folders = glob.glob(first_folder + '/*')\n",
    "\n",
    "    for second_folder in second_folders:\n",
    "    \n",
    "        third_folders = glob.glob(second_folder + '/*.flac')\n",
    "        \n",
    "        txt_folder = second_folder.replace('./converted_sound_Waveunet_test/*.flac/', './Datasets/audio_to_anon/LibriSpeech/dev-clean/')\n",
    "        text_file = glob.glob(txt_folder + '/*.txt')[0]\n",
    "        \n",
    "        texts_df = load_txt(text_file)\n",
    "        \n",
    "        for audio_path in third_folders:\n",
    "            prediction = transcribe_audio(audio_path)[0]\n",
    "            \n",
    "            code = re.search(r'/([^/]+)\\.flac$', audio_path).group(1)\n",
    "            reference = texts_df.loc[texts_df.code == code].transcription.tolist()[0]\n",
    "            \n",
    "            # print(f'Reference: {reference.lower()}')\n",
    "            # print(f'Prediction: {prediction[0].lower()}')\n",
    "            \n",
    "            wer_score += wer(reference.lower(), prediction.lower())\n",
    "            num_files += 1\n",
    "\n",
    "average_wer = wer_score / num_files\n",
    "print(f\"Average WER: {average_wer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ca1e305-5c19-4df9-b493-b470ce381113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/vcloak/lib/python3.9/site-packages/transformers/generation/utils.py:1298: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/conda/envs/vcloak/lib/python3.9/site-packages/transformers/generation/utils.py:1298: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/conda/envs/vcloak/lib/python3.9/site-packages/transformers/generation/utils.py:1298: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/user/conda/envs/vcloak/lib/python3.9/site-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average WER: 0.28955901365841685\n"
     ]
    }
   ],
   "source": [
    "from jiwer import wer\n",
    "\n",
    "first_folders = glob.glob('./processed/*')\n",
    "\n",
    "wer_score = 0\n",
    "num_files = 0\n",
    "\n",
    "for i, first_folder in enumerate(first_folders):\n",
    "    clear_output(wait=False)\n",
    "    print(i)\n",
    "    \n",
    "    second_folders = glob.glob(first_folder + '/*')\n",
    "\n",
    "    for second_folder in second_folders:\n",
    "    \n",
    "        third_folders = glob.glob(second_folder + '/*.flac')\n",
    "        \n",
    "        txt_folder = second_folder.replace('./processed/', './Datasets/audio_to_anon/LibriSpeech/dev-clean/')\n",
    "        text_file = glob.glob(txt_folder + '/*.txt')[0]\n",
    "        \n",
    "        texts_df = load_txt(text_file)\n",
    "        \n",
    "        for audio_path in third_folders:\n",
    "            prediction = transcribe_audio(audio_path)[0]\n",
    "            \n",
    "            code = re.search(r'/([^/]+)\\.flac$', audio_path).group(1)\n",
    "            reference = texts_df.loc[texts_df.code == code].transcription.tolist()[0]\n",
    "            \n",
    "            # print(f'Reference: {reference.lower()}')\n",
    "            # print(f'Prediction: {prediction[0].lower()}')\n",
    "            \n",
    "            wer_score += wer(reference.lower(), prediction.lower())\n",
    "            num_files += 1\n",
    "\n",
    "average_wer = wer_score / num_files\n",
    "print(f\"Average WER: {average_wer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vcloak",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
